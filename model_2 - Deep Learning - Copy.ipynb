{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the CSV and Perform Basic Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>koi_disposition</th>\n",
       "      <th>koi_fpflag_nt</th>\n",
       "      <th>koi_fpflag_ss</th>\n",
       "      <th>koi_fpflag_co</th>\n",
       "      <th>koi_fpflag_ec</th>\n",
       "      <th>koi_period</th>\n",
       "      <th>koi_period_err1</th>\n",
       "      <th>koi_period_err2</th>\n",
       "      <th>koi_time0bk</th>\n",
       "      <th>koi_time0bk_err1</th>\n",
       "      <th>...</th>\n",
       "      <th>koi_steff_err2</th>\n",
       "      <th>koi_slogg</th>\n",
       "      <th>koi_slogg_err1</th>\n",
       "      <th>koi_slogg_err2</th>\n",
       "      <th>koi_srad</th>\n",
       "      <th>koi_srad_err1</th>\n",
       "      <th>koi_srad_err2</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>koi_kepmag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CONFIRMED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>54.418383</td>\n",
       "      <td>2.479000e-04</td>\n",
       "      <td>-2.479000e-04</td>\n",
       "      <td>162.513840</td>\n",
       "      <td>0.003520</td>\n",
       "      <td>...</td>\n",
       "      <td>-81</td>\n",
       "      <td>4.467</td>\n",
       "      <td>0.064</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>0.927</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>291.93423</td>\n",
       "      <td>48.141651</td>\n",
       "      <td>15.347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.899140</td>\n",
       "      <td>1.490000e-05</td>\n",
       "      <td>-1.490000e-05</td>\n",
       "      <td>175.850252</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>...</td>\n",
       "      <td>-176</td>\n",
       "      <td>4.544</td>\n",
       "      <td>0.044</td>\n",
       "      <td>-0.176</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.233</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>297.00482</td>\n",
       "      <td>48.134129</td>\n",
       "      <td>15.436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FALSE POSITIVE</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.736952</td>\n",
       "      <td>2.630000e-07</td>\n",
       "      <td>-2.630000e-07</td>\n",
       "      <td>170.307565</td>\n",
       "      <td>0.000115</td>\n",
       "      <td>...</td>\n",
       "      <td>-174</td>\n",
       "      <td>4.564</td>\n",
       "      <td>0.053</td>\n",
       "      <td>-0.168</td>\n",
       "      <td>0.791</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>285.53461</td>\n",
       "      <td>48.285210</td>\n",
       "      <td>15.597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CONFIRMED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.525592</td>\n",
       "      <td>3.760000e-06</td>\n",
       "      <td>-3.760000e-06</td>\n",
       "      <td>171.595550</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>...</td>\n",
       "      <td>-211</td>\n",
       "      <td>4.438</td>\n",
       "      <td>0.070</td>\n",
       "      <td>-0.210</td>\n",
       "      <td>1.046</td>\n",
       "      <td>0.334</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>288.75488</td>\n",
       "      <td>48.226200</td>\n",
       "      <td>15.509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CONFIRMED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.134435</td>\n",
       "      <td>1.050000e-05</td>\n",
       "      <td>-1.050000e-05</td>\n",
       "      <td>172.979370</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>...</td>\n",
       "      <td>-232</td>\n",
       "      <td>4.486</td>\n",
       "      <td>0.054</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.315</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>296.28613</td>\n",
       "      <td>48.224670</td>\n",
       "      <td>15.714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  koi_disposition  koi_fpflag_nt  koi_fpflag_ss  koi_fpflag_co  koi_fpflag_ec  \\\n",
       "0       CONFIRMED              0              0              0              0   \n",
       "1  FALSE POSITIVE              0              1              0              0   \n",
       "2  FALSE POSITIVE              0              1              0              0   \n",
       "3       CONFIRMED              0              0              0              0   \n",
       "4       CONFIRMED              0              0              0              0   \n",
       "\n",
       "   koi_period  koi_period_err1  koi_period_err2  koi_time0bk  \\\n",
       "0   54.418383     2.479000e-04    -2.479000e-04   162.513840   \n",
       "1   19.899140     1.490000e-05    -1.490000e-05   175.850252   \n",
       "2    1.736952     2.630000e-07    -2.630000e-07   170.307565   \n",
       "3    2.525592     3.760000e-06    -3.760000e-06   171.595550   \n",
       "4    4.134435     1.050000e-05    -1.050000e-05   172.979370   \n",
       "\n",
       "   koi_time0bk_err1  ...  koi_steff_err2  koi_slogg  koi_slogg_err1  \\\n",
       "0          0.003520  ...             -81      4.467           0.064   \n",
       "1          0.000581  ...            -176      4.544           0.044   \n",
       "2          0.000115  ...            -174      4.564           0.053   \n",
       "3          0.001130  ...            -211      4.438           0.070   \n",
       "4          0.001900  ...            -232      4.486           0.054   \n",
       "\n",
       "   koi_slogg_err2  koi_srad  koi_srad_err1  koi_srad_err2         ra  \\\n",
       "0          -0.096     0.927          0.105         -0.061  291.93423   \n",
       "1          -0.176     0.868          0.233         -0.078  297.00482   \n",
       "2          -0.168     0.791          0.201         -0.067  285.53461   \n",
       "3          -0.210     1.046          0.334         -0.133  288.75488   \n",
       "4          -0.229     0.972          0.315         -0.105  296.28613   \n",
       "\n",
       "         dec  koi_kepmag  \n",
       "0  48.141651      15.347  \n",
       "1  48.134129      15.436  \n",
       "2  48.285210      15.597  \n",
       "3  48.226200      15.509  \n",
       "4  48.224670      15.714  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"exoplanet_data.csv\")\n",
    "# Drop the null columns where all values are null\n",
    "df = df.dropna(axis='columns', how='all')\n",
    "# Drop the null rows\n",
    "df = df.dropna()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['koi_disposition', 'koi_fpflag_nt', 'koi_fpflag_ss', 'koi_fpflag_co',\n",
       "       'koi_fpflag_ec', 'koi_period', 'koi_period_err1', 'koi_period_err2',\n",
       "       'koi_time0bk', 'koi_time0bk_err1', 'koi_time0bk_err2', 'koi_impact',\n",
       "       'koi_impact_err1', 'koi_impact_err2', 'koi_duration',\n",
       "       'koi_duration_err1', 'koi_duration_err2', 'koi_depth', 'koi_depth_err1',\n",
       "       'koi_depth_err2', 'koi_prad', 'koi_prad_err1', 'koi_prad_err2',\n",
       "       'koi_teq', 'koi_insol', 'koi_insol_err1', 'koi_insol_err2',\n",
       "       'koi_model_snr', 'koi_tce_plnt_num', 'koi_steff', 'koi_steff_err1',\n",
       "       'koi_steff_err2', 'koi_slogg', 'koi_slogg_err1', 'koi_slogg_err2',\n",
       "       'koi_srad', 'koi_srad_err1', 'koi_srad_err2', 'ra', 'dec',\n",
       "       'koi_kepmag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop unnecesary columns for the model \n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the error columns \n",
    "df = df.drop(['koi_period_err1', 'koi_period_err2', 'koi_time0bk_err1', 'koi_time0bk_err2',\n",
    "              'koi_impact_err1', 'koi_impact_err2', 'koi_duration_err1', 'koi_duration_err2',\n",
    "              'koi_depth_err1', 'koi_depth_err2', 'koi_prad_err1', 'koi_prad_err2', \n",
    "              'koi_insol_err1', 'koi_insol_err2', 'koi_steff_err1', 'koi_steff_err2', \n",
    "              'koi_slogg_err1', 'koi_slogg_err2', 'koi_srad_err1', 'koi_srad_err2'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Definitions\n",
    "#### <b>source:</b> <a href=\"https://exoplanetarchive.ipac.caltech.edu/docs/API_kepcandidate_columns.html\">NASA Exoplanet Archive</a>  \n",
    " \n",
    "<strong>koi_fpflag_nt</strong>  \n",
    "A KOI whose light curve is not consistent with that of a transiting planet. This includes, but is not limited to, instrumental artifacts, non-eclipsing variable stars, and spurious (very low SNR) detections  \n",
    "<strong>koi_fpflag_ss</strong>  \n",
    "A KOI that is observed to have a significant secondary event, transit shape, or out-of-eclipse variability, which indicates that the transit-like event is most likely caused by an eclipsing binary. However, self-luminous, hot Jupiters with a visible secondary eclipse will also have this flag set, but with a disposition of PC.  \n",
    "<strong>koi_fpflag_co</strong>  \n",
    "The source of the signal is from a nearby star, as inferred by measuring the centroid location of the image both in and out of transit, or by the strength of the transit signal in the target's outer (halo) pixels as compared to the transit signal from the pixels in the optimal (or core) aperture.  \n",
    "<strong>koi_fpflag_ec</strong>    \n",
    "The KOI shares the same period and epoch as another object and is judged to be the result of flux contamination in the aperture or electronic crosstalk.    \n",
    "<strong>koi_period</strong>    \n",
    "The interval between consecutive planetary transits.  \n",
    "<strong>koi_period_err1</strong>  \n",
    "Level of uncertainty for above  \n",
    "<strong>koi_period_err2</strong>   \n",
    "Level of uncertainty for above  \n",
    "<strong>koi_time0bk</strong>  \n",
    "The time corresponding to the center of the first detected transit in Barycentric Julian Day (BJD) minus a constant offset of 2,454,833.0 days. The offset corresponds to 12:00 on Jan 1, 2009 UTC.  \n",
    "<strong>koi_time0bk_err1</strong>  \n",
    "Level of uncertainty for above  \n",
    "<strong>koi_time0bk_err2</strong>  \n",
    "Level of uncertainty for above  \n",
    "<strong>koi_impact</strong>  \n",
    "The sky-projected distance between the center of the stellar disc and the center of the planet disc at conjunction, normalized by the stellar radius.  \n",
    "<strong>koi_impact_err1</strong>  \n",
    "Level of uncertainty for above  \n",
    "<strong>koi_impact_err2</strong>  \n",
    "Level of uncertainty for above  \n",
    "<strong>koi_duration</strong>  \n",
    "The duration of the observed transits. Duration is measured from first contact between the planet and star until last contact. Contact times are typically computed from a best-fit model produced by a Mandel-Agol (2002) model fit to a multi-quarter Kepler light curve, assuming a linear orbital ephemeris.  \n",
    "<strong>koi_duration_err1</strong>  \n",
    "Level of uncertainty for above  \n",
    "<strong>koi_duration_err2</strong>  \n",
    "Level of uncertainty for above  \n",
    "<strong>koi_depth</strong>  \n",
    "The fraction of stellar flux lost at the minimum of the planetary transit. Transit depths are typically computed from a best-fit model produced by a Mandel-Agol (2002) model fit to a multi-quarter Kepler light curve, assuming a linear orbital ephemeris.  \n",
    "<strong>koi_depth_err1</strong>  \n",
    "Level of uncertainty for above  \n",
    "<strong>koi_depth_err2</strong>  \n",
    "Level of uncertainty for above  \n",
    "<strong>koi_prad</strong>  \n",
    "The radius of the planet. Planetary radius is the product of the planet star radius ratio and the stellar radius.  \n",
    "<strong>koi_prad_err1</strong>  \n",
    "Level of uncertainty for above  \n",
    "<strong>koi_prad_err2</strong>  \n",
    "Level of uncertainty for above  \n",
    "<strong>koi_teq</strong>  \n",
    "Approximation for the temperature of the planet. The calculation of equilibrium temperature assumes a) thermodynamic equilibrium between the incident stellar flux and the radiated heat from the planet, b) a Bond albedo (the fraction of total power incident upon the planet scattered back into space) of 0.3, c) the planet and star are blackbodies, and d) the heat is evenly distributed between the day and night sides of the planet.  \n",
    "<strong>koi_insol</strong>  \n",
    "Insolation flux is another way to give the equilibrium temperature. It depends on the stellar parameters (specifically the stellar radius and temperature), and on the semi-major axis of the planet. It's given in units relative to those measured for the Earth from the Sun.  \n",
    "<strong>koi_insol_err1</strong>  \n",
    "Level of uncertainty for above  \n",
    "<strong>koi_insol_err2</strong>  \n",
    "Level of uncertainty for above  \n",
    "<strong>koi_model_snr</strong>  \n",
    "Transit depth normalized by the mean uncertainty in the flux during the transits.  \n",
    "<strong>koi_tce_plnt_num</strong>  \n",
    "TCE Planet Number federated to the KOI.  \n",
    "<strong>koi_steff</strong>  \n",
    "The photospheric temperature of the star.  \n",
    "<strong>koi_steff_err1</strong>  \n",
    "Level of uncertainty for above  \n",
    "<strong>koi_steff_err2</strong>  \n",
    "Level of uncertainty for above  \n",
    "<strong>koi_slogg</strong>  \n",
    "The base-10 logarithm of the acceleration due to gravity at the surface of the star.  \n",
    "<strong>koi_slogg_err1</strong>  \n",
    "Level of uncertainty for above  \n",
    "<strong>koi_slogg_err2</strong>  \n",
    "Level of uncertainty for above  \n",
    "<strong>koi_srad</strong>  \n",
    "The photospheric radius of the star  \n",
    "<strong>koi_srad_err1</strong>  \n",
    "Level of uncertainty for above  \n",
    "<strong>koi_srad_err2</strong>  \n",
    "Level of uncertainty for above  \n",
    "<strong>ra</strong>  \n",
    "KIC Right Ascension  \n",
    "<strong>dec</strong>  \n",
    "KIC Declination  \n",
    "<strong>koi_kepmag</strong>    \n",
    "Kepler-band (mag)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set features\n",
    "# Initially select all but the error columns\n",
    "\n",
    "feature_names = ['koi_fpflag_nt', 'koi_fpflag_ss', 'koi_fpflag_co', 'koi_fpflag_ec', 'koi_period',\n",
    "                 'koi_time0bk', 'koi_impact', 'koi_duration', 'koi_depth', 'koi_prad', 'koi_teq', 'koi_insol',\n",
    "                 'koi_model_snr', 'koi_tce_plnt_num', 'koi_steff','koi_slogg','koi_srad', 'ra', 'dec', 'koi_kepmag']\n",
    "\n",
    "X = df[feature_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CONFIRMED', 'FALSE POSITIVE', 'CANDIDATE'], dtype=object)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_names = df['koi_disposition'].unique()[0:3]\n",
    "target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "y  = df[\"koi_disposition\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Train Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>koi_fpflag_nt</th>\n",
       "      <th>koi_fpflag_ss</th>\n",
       "      <th>koi_fpflag_co</th>\n",
       "      <th>koi_fpflag_ec</th>\n",
       "      <th>koi_period</th>\n",
       "      <th>koi_time0bk</th>\n",
       "      <th>koi_impact</th>\n",
       "      <th>koi_duration</th>\n",
       "      <th>koi_depth</th>\n",
       "      <th>koi_prad</th>\n",
       "      <th>koi_teq</th>\n",
       "      <th>koi_insol</th>\n",
       "      <th>koi_model_snr</th>\n",
       "      <th>koi_tce_plnt_num</th>\n",
       "      <th>koi_steff</th>\n",
       "      <th>koi_slogg</th>\n",
       "      <th>koi_srad</th>\n",
       "      <th>ra</th>\n",
       "      <th>dec</th>\n",
       "      <th>koi_kepmag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6122</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.768901</td>\n",
       "      <td>133.077240</td>\n",
       "      <td>0.150</td>\n",
       "      <td>3.61600</td>\n",
       "      <td>123.1</td>\n",
       "      <td>1.24</td>\n",
       "      <td>1017</td>\n",
       "      <td>253.30</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1</td>\n",
       "      <td>5737</td>\n",
       "      <td>4.327</td>\n",
       "      <td>1.125</td>\n",
       "      <td>294.40472</td>\n",
       "      <td>39.351681</td>\n",
       "      <td>14.725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6370</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.733726</td>\n",
       "      <td>132.020050</td>\n",
       "      <td>0.291</td>\n",
       "      <td>2.30900</td>\n",
       "      <td>114.6</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1867</td>\n",
       "      <td>2891.64</td>\n",
       "      <td>13.8</td>\n",
       "      <td>1</td>\n",
       "      <td>5855</td>\n",
       "      <td>4.578</td>\n",
       "      <td>0.797</td>\n",
       "      <td>284.50391</td>\n",
       "      <td>42.463860</td>\n",
       "      <td>15.770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2879</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.652707</td>\n",
       "      <td>134.460380</td>\n",
       "      <td>0.970</td>\n",
       "      <td>79.89690</td>\n",
       "      <td>641.1</td>\n",
       "      <td>3.21</td>\n",
       "      <td>989</td>\n",
       "      <td>226.81</td>\n",
       "      <td>254.3</td>\n",
       "      <td>1</td>\n",
       "      <td>6328</td>\n",
       "      <td>4.481</td>\n",
       "      <td>0.963</td>\n",
       "      <td>295.50211</td>\n",
       "      <td>38.983540</td>\n",
       "      <td>13.099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.953547</td>\n",
       "      <td>174.662240</td>\n",
       "      <td>0.300</td>\n",
       "      <td>2.63120</td>\n",
       "      <td>875.4</td>\n",
       "      <td>2.25</td>\n",
       "      <td>696</td>\n",
       "      <td>55.37</td>\n",
       "      <td>38.4</td>\n",
       "      <td>1</td>\n",
       "      <td>4768</td>\n",
       "      <td>4.536</td>\n",
       "      <td>0.779</td>\n",
       "      <td>291.15878</td>\n",
       "      <td>40.750271</td>\n",
       "      <td>15.660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.959319</td>\n",
       "      <td>172.258529</td>\n",
       "      <td>0.831</td>\n",
       "      <td>2.22739</td>\n",
       "      <td>9802.0</td>\n",
       "      <td>12.21</td>\n",
       "      <td>1103</td>\n",
       "      <td>349.40</td>\n",
       "      <td>696.5</td>\n",
       "      <td>1</td>\n",
       "      <td>5712</td>\n",
       "      <td>4.359</td>\n",
       "      <td>1.082</td>\n",
       "      <td>292.16705</td>\n",
       "      <td>48.727589</td>\n",
       "      <td>15.263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      koi_fpflag_nt  koi_fpflag_ss  koi_fpflag_co  koi_fpflag_ec  koi_period  \\\n",
       "6122              0              0              0              0    6.768901   \n",
       "6370              0              1              0              1    0.733726   \n",
       "2879              1              0              0              0    7.652707   \n",
       "107               0              0              0              0    7.953547   \n",
       "29                0              0              0              0    4.959319   \n",
       "\n",
       "      koi_time0bk  koi_impact  koi_duration  koi_depth  koi_prad  koi_teq  \\\n",
       "6122   133.077240       0.150       3.61600      123.1      1.24     1017   \n",
       "6370   132.020050       0.291       2.30900      114.6      0.86     1867   \n",
       "2879   134.460380       0.970      79.89690      641.1      3.21      989   \n",
       "107    174.662240       0.300       2.63120      875.4      2.25      696   \n",
       "29     172.258529       0.831       2.22739     9802.0     12.21     1103   \n",
       "\n",
       "      koi_insol  koi_model_snr  koi_tce_plnt_num  koi_steff  koi_slogg  \\\n",
       "6122     253.30           10.8                 1       5737      4.327   \n",
       "6370    2891.64           13.8                 1       5855      4.578   \n",
       "2879     226.81          254.3                 1       6328      4.481   \n",
       "107       55.37           38.4                 1       4768      4.536   \n",
       "29       349.40          696.5                 1       5712      4.359   \n",
       "\n",
       "      koi_srad         ra        dec  koi_kepmag  \n",
       "6122     1.125  294.40472  39.351681      14.725  \n",
       "6370     0.797  284.50391  42.463860      15.770  \n",
       "2879     0.963  295.50211  38.983540      13.099  \n",
       "107      0.779  291.15878  40.750271      15.660  \n",
       "29       1.082  292.16705  48.727589      15.263  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "Scale the data using the MinMaxScaler and perform some feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create the scaler\n",
    "X_scaler = MinMaxScaler().fit(X_train)\n",
    "\n",
    "# Transform the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the y values to numerical using label_encoder/to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "encoded_y_train = label_encoder.transform(y_train)\n",
    "encoded_y_test = label_encoder.transform(y_test)\n",
    "\n",
    "y_train_categorical = to_categorical(encoded_y_train)\n",
    "y_test_categorical = to_categorical(encoded_y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and add layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "inputs = X_train.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=100, activation='relu', input_dim=inputs))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=3, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and fit the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 100)               2100      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 12,503\n",
      "Trainable params: 12,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    " model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "164/164 - 0s - loss: 0.5079 - accuracy: 0.7358\n",
      "Epoch 2/200\n",
      "164/164 - 0s - loss: 0.3768 - accuracy: 0.7906\n",
      "Epoch 3/200\n",
      "164/164 - 0s - loss: 0.3694 - accuracy: 0.8003\n",
      "Epoch 4/200\n",
      "164/164 - 0s - loss: 0.3673 - accuracy: 0.7952\n",
      "Epoch 5/200\n",
      "164/164 - 0s - loss: 0.3599 - accuracy: 0.8108\n",
      "Epoch 6/200\n",
      "164/164 - 0s - loss: 0.3597 - accuracy: 0.8104\n",
      "Epoch 7/200\n",
      "164/164 - 0s - loss: 0.3554 - accuracy: 0.8119\n",
      "Epoch 8/200\n",
      "164/164 - 0s - loss: 0.3518 - accuracy: 0.8169\n",
      "Epoch 9/200\n",
      "164/164 - 0s - loss: 0.3487 - accuracy: 0.8135\n",
      "Epoch 10/200\n",
      "164/164 - 0s - loss: 0.3503 - accuracy: 0.8192\n",
      "Epoch 11/200\n",
      "164/164 - 0s - loss: 0.3472 - accuracy: 0.8194\n",
      "Epoch 12/200\n",
      "164/164 - 0s - loss: 0.3466 - accuracy: 0.8203\n",
      "Epoch 13/200\n",
      "164/164 - 0s - loss: 0.3425 - accuracy: 0.8253\n",
      "Epoch 14/200\n",
      "164/164 - 0s - loss: 0.3411 - accuracy: 0.8262\n",
      "Epoch 15/200\n",
      "164/164 - 0s - loss: 0.3399 - accuracy: 0.8276\n",
      "Epoch 16/200\n",
      "164/164 - 0s - loss: 0.3393 - accuracy: 0.8272\n",
      "Epoch 17/200\n",
      "164/164 - 0s - loss: 0.3389 - accuracy: 0.8282\n",
      "Epoch 18/200\n",
      "164/164 - 0s - loss: 0.3394 - accuracy: 0.8253\n",
      "Epoch 19/200\n",
      "164/164 - 0s - loss: 0.3354 - accuracy: 0.8291\n",
      "Epoch 20/200\n",
      "164/164 - 0s - loss: 0.3366 - accuracy: 0.8285\n",
      "Epoch 21/200\n",
      "164/164 - 0s - loss: 0.3356 - accuracy: 0.8295\n",
      "Epoch 22/200\n",
      "164/164 - 0s - loss: 0.3302 - accuracy: 0.8352\n",
      "Epoch 23/200\n",
      "164/164 - 0s - loss: 0.3302 - accuracy: 0.8320\n",
      "Epoch 24/200\n",
      "164/164 - 0s - loss: 0.3279 - accuracy: 0.8327\n",
      "Epoch 25/200\n",
      "164/164 - 0s - loss: 0.3283 - accuracy: 0.8327\n",
      "Epoch 26/200\n",
      "164/164 - 0s - loss: 0.3249 - accuracy: 0.8346\n",
      "Epoch 27/200\n",
      "164/164 - 0s - loss: 0.3276 - accuracy: 0.8348\n",
      "Epoch 28/200\n",
      "164/164 - 0s - loss: 0.3246 - accuracy: 0.8358\n",
      "Epoch 29/200\n",
      "164/164 - 0s - loss: 0.3227 - accuracy: 0.8354\n",
      "Epoch 30/200\n",
      "164/164 - 0s - loss: 0.3221 - accuracy: 0.8367\n",
      "Epoch 31/200\n",
      "164/164 - 0s - loss: 0.3193 - accuracy: 0.8369\n",
      "Epoch 32/200\n",
      "164/164 - 0s - loss: 0.3197 - accuracy: 0.8394\n",
      "Epoch 33/200\n",
      "164/164 - 0s - loss: 0.3148 - accuracy: 0.8405\n",
      "Epoch 34/200\n",
      "164/164 - 0s - loss: 0.3152 - accuracy: 0.8405\n",
      "Epoch 35/200\n",
      "164/164 - 0s - loss: 0.3125 - accuracy: 0.8407\n",
      "Epoch 36/200\n",
      "164/164 - 0s - loss: 0.3090 - accuracy: 0.8474\n",
      "Epoch 37/200\n",
      "164/164 - 0s - loss: 0.3149 - accuracy: 0.8358\n",
      "Epoch 38/200\n",
      "164/164 - 0s - loss: 0.3110 - accuracy: 0.8446\n",
      "Epoch 39/200\n",
      "164/164 - 0s - loss: 0.3084 - accuracy: 0.8478\n",
      "Epoch 40/200\n",
      "164/164 - 0s - loss: 0.3063 - accuracy: 0.8505\n",
      "Epoch 41/200\n",
      "164/164 - 0s - loss: 0.3067 - accuracy: 0.8478\n",
      "Epoch 42/200\n",
      "164/164 - 0s - loss: 0.3033 - accuracy: 0.8529\n",
      "Epoch 43/200\n",
      "164/164 - 0s - loss: 0.3027 - accuracy: 0.8476\n",
      "Epoch 44/200\n",
      "164/164 - 0s - loss: 0.3013 - accuracy: 0.8516\n",
      "Epoch 45/200\n",
      "164/164 - 0s - loss: 0.2995 - accuracy: 0.8512\n",
      "Epoch 46/200\n",
      "164/164 - 0s - loss: 0.2996 - accuracy: 0.8503\n",
      "Epoch 47/200\n",
      "164/164 - 0s - loss: 0.2970 - accuracy: 0.8510\n",
      "Epoch 48/200\n",
      "164/164 - 0s - loss: 0.2950 - accuracy: 0.8550\n",
      "Epoch 49/200\n",
      "164/164 - 0s - loss: 0.2970 - accuracy: 0.8562\n",
      "Epoch 50/200\n",
      "164/164 - 0s - loss: 0.2906 - accuracy: 0.8592\n",
      "Epoch 51/200\n",
      "164/164 - 0s - loss: 0.2924 - accuracy: 0.8575\n",
      "Epoch 52/200\n",
      "164/164 - 0s - loss: 0.2892 - accuracy: 0.8581\n",
      "Epoch 53/200\n",
      "164/164 - 0s - loss: 0.2877 - accuracy: 0.8589\n",
      "Epoch 54/200\n",
      "164/164 - 0s - loss: 0.2863 - accuracy: 0.8606\n",
      "Epoch 55/200\n",
      "164/164 - 0s - loss: 0.2876 - accuracy: 0.8568\n",
      "Epoch 56/200\n",
      "164/164 - 0s - loss: 0.2920 - accuracy: 0.8518\n",
      "Epoch 57/200\n",
      "164/164 - 0s - loss: 0.2852 - accuracy: 0.8583\n",
      "Epoch 58/200\n",
      "164/164 - 0s - loss: 0.2810 - accuracy: 0.8617\n",
      "Epoch 59/200\n",
      "164/164 - 0s - loss: 0.2860 - accuracy: 0.8552\n",
      "Epoch 60/200\n",
      "164/164 - 0s - loss: 0.2799 - accuracy: 0.8667\n",
      "Epoch 61/200\n",
      "164/164 - 0s - loss: 0.2834 - accuracy: 0.8600\n",
      "Epoch 62/200\n",
      "164/164 - 0s - loss: 0.2773 - accuracy: 0.8623\n",
      "Epoch 63/200\n",
      "164/164 - 0s - loss: 0.2777 - accuracy: 0.8642\n",
      "Epoch 64/200\n",
      "164/164 - 0s - loss: 0.2756 - accuracy: 0.8705\n",
      "Epoch 65/200\n",
      "164/164 - 0s - loss: 0.2734 - accuracy: 0.8684\n",
      "Epoch 66/200\n",
      "164/164 - 0s - loss: 0.2726 - accuracy: 0.8636\n",
      "Epoch 67/200\n",
      "164/164 - 0s - loss: 0.2701 - accuracy: 0.8722\n",
      "Epoch 68/200\n",
      "164/164 - 0s - loss: 0.2704 - accuracy: 0.8705\n",
      "Epoch 69/200\n",
      "164/164 - 0s - loss: 0.2652 - accuracy: 0.8720\n",
      "Epoch 70/200\n",
      "164/164 - 0s - loss: 0.2647 - accuracy: 0.8743\n",
      "Epoch 71/200\n",
      "164/164 - 0s - loss: 0.2657 - accuracy: 0.8728\n",
      "Epoch 72/200\n",
      "164/164 - 0s - loss: 0.2655 - accuracy: 0.8734\n",
      "Epoch 73/200\n",
      "164/164 - 0s - loss: 0.2641 - accuracy: 0.8781\n",
      "Epoch 74/200\n",
      "164/164 - 0s - loss: 0.2602 - accuracy: 0.8758\n",
      "Epoch 75/200\n",
      "164/164 - 0s - loss: 0.2624 - accuracy: 0.8760\n",
      "Epoch 76/200\n",
      "164/164 - 0s - loss: 0.2640 - accuracy: 0.8705\n",
      "Epoch 77/200\n",
      "164/164 - 0s - loss: 0.2580 - accuracy: 0.8749\n",
      "Epoch 78/200\n",
      "164/164 - 0s - loss: 0.2569 - accuracy: 0.8755\n",
      "Epoch 79/200\n",
      "164/164 - 0s - loss: 0.2554 - accuracy: 0.8768\n",
      "Epoch 80/200\n",
      "164/164 - 0s - loss: 0.2536 - accuracy: 0.8840\n",
      "Epoch 81/200\n",
      "164/164 - 0s - loss: 0.2507 - accuracy: 0.8854\n",
      "Epoch 82/200\n",
      "164/164 - 0s - loss: 0.2506 - accuracy: 0.8840\n",
      "Epoch 83/200\n",
      "164/164 - 0s - loss: 0.2542 - accuracy: 0.8823\n",
      "Epoch 84/200\n",
      "164/164 - 0s - loss: 0.2478 - accuracy: 0.8838\n",
      "Epoch 85/200\n",
      "164/164 - 0s - loss: 0.2488 - accuracy: 0.8835\n",
      "Epoch 86/200\n",
      "164/164 - 0s - loss: 0.2477 - accuracy: 0.8852\n",
      "Epoch 87/200\n",
      "164/164 - 0s - loss: 0.2480 - accuracy: 0.8861\n",
      "Epoch 88/200\n",
      "164/164 - 0s - loss: 0.2443 - accuracy: 0.8852\n",
      "Epoch 89/200\n",
      "164/164 - 0s - loss: 0.2484 - accuracy: 0.8884\n",
      "Epoch 90/200\n",
      "164/164 - 0s - loss: 0.2395 - accuracy: 0.8943\n",
      "Epoch 91/200\n",
      "164/164 - 0s - loss: 0.2429 - accuracy: 0.8848\n",
      "Epoch 92/200\n",
      "164/164 - 0s - loss: 0.2400 - accuracy: 0.8943\n",
      "Epoch 93/200\n",
      "164/164 - 0s - loss: 0.2413 - accuracy: 0.8848\n",
      "Epoch 94/200\n",
      "164/164 - 0s - loss: 0.2388 - accuracy: 0.8957\n",
      "Epoch 95/200\n",
      "164/164 - 0s - loss: 0.2309 - accuracy: 0.8966\n",
      "Epoch 96/200\n",
      "164/164 - 0s - loss: 0.2324 - accuracy: 0.8962\n",
      "Epoch 97/200\n",
      "164/164 - 0s - loss: 0.2378 - accuracy: 0.8903\n",
      "Epoch 98/200\n",
      "164/164 - 0s - loss: 0.2317 - accuracy: 0.8962\n",
      "Epoch 99/200\n",
      "164/164 - 0s - loss: 0.2308 - accuracy: 0.8983\n",
      "Epoch 100/200\n",
      "164/164 - 0s - loss: 0.2364 - accuracy: 0.8949\n",
      "Epoch 101/200\n",
      "164/164 - 0s - loss: 0.2348 - accuracy: 0.8957\n",
      "Epoch 102/200\n",
      "164/164 - 0s - loss: 0.2303 - accuracy: 0.8968\n",
      "Epoch 103/200\n",
      "164/164 - 0s - loss: 0.2273 - accuracy: 0.8989\n",
      "Epoch 104/200\n",
      "164/164 - 0s - loss: 0.2277 - accuracy: 0.8964\n",
      "Epoch 105/200\n",
      "164/164 - 0s - loss: 0.2266 - accuracy: 0.8997\n",
      "Epoch 106/200\n",
      "164/164 - 0s - loss: 0.2231 - accuracy: 0.9008\n",
      "Epoch 107/200\n",
      "164/164 - 0s - loss: 0.2274 - accuracy: 0.8985\n",
      "Epoch 108/200\n",
      "164/164 - 0s - loss: 0.2271 - accuracy: 0.9001\n",
      "Epoch 109/200\n",
      "164/164 - 0s - loss: 0.2228 - accuracy: 0.9020\n",
      "Epoch 110/200\n",
      "164/164 - 0s - loss: 0.2233 - accuracy: 0.8991\n",
      "Epoch 111/200\n",
      "164/164 - 0s - loss: 0.2241 - accuracy: 0.9004\n",
      "Epoch 112/200\n",
      "164/164 - 0s - loss: 0.2164 - accuracy: 0.9027\n",
      "Epoch 113/200\n",
      "164/164 - 0s - loss: 0.2240 - accuracy: 0.9029\n",
      "Epoch 114/200\n",
      "164/164 - 0s - loss: 0.2217 - accuracy: 0.8997\n",
      "Epoch 115/200\n",
      "164/164 - 0s - loss: 0.2170 - accuracy: 0.9029\n",
      "Epoch 116/200\n",
      "164/164 - 0s - loss: 0.2156 - accuracy: 0.9062\n",
      "Epoch 117/200\n",
      "164/164 - 0s - loss: 0.2130 - accuracy: 0.9092\n",
      "Epoch 118/200\n",
      "164/164 - 0s - loss: 0.2183 - accuracy: 0.9012\n",
      "Epoch 119/200\n",
      "164/164 - 0s - loss: 0.2160 - accuracy: 0.9062\n",
      "Epoch 120/200\n",
      "164/164 - 0s - loss: 0.2130 - accuracy: 0.9086\n",
      "Epoch 121/200\n",
      "164/164 - 0s - loss: 0.2109 - accuracy: 0.9090\n",
      "Epoch 122/200\n",
      "164/164 - 0s - loss: 0.2131 - accuracy: 0.9071\n",
      "Epoch 123/200\n",
      "164/164 - 0s - loss: 0.2078 - accuracy: 0.9115\n",
      "Epoch 124/200\n",
      "164/164 - 0s - loss: 0.2094 - accuracy: 0.9100\n",
      "Epoch 125/200\n",
      "164/164 - 0s - loss: 0.2123 - accuracy: 0.9044\n",
      "Epoch 126/200\n",
      "164/164 - 0s - loss: 0.2073 - accuracy: 0.9100\n",
      "Epoch 127/200\n",
      "164/164 - 0s - loss: 0.2043 - accuracy: 0.9138\n",
      "Epoch 128/200\n",
      "164/164 - 0s - loss: 0.2077 - accuracy: 0.9115\n",
      "Epoch 129/200\n",
      "164/164 - 0s - loss: 0.2092 - accuracy: 0.9117\n",
      "Epoch 130/200\n",
      "164/164 - 0s - loss: 0.2120 - accuracy: 0.9014\n",
      "Epoch 131/200\n",
      "164/164 - 0s - loss: 0.2040 - accuracy: 0.9119\n",
      "Epoch 132/200\n",
      "164/164 - 0s - loss: 0.2029 - accuracy: 0.9100\n",
      "Epoch 133/200\n",
      "164/164 - 0s - loss: 0.1974 - accuracy: 0.9172\n",
      "Epoch 134/200\n",
      "164/164 - 0s - loss: 0.2019 - accuracy: 0.9130\n",
      "Epoch 135/200\n",
      "164/164 - 0s - loss: 0.2047 - accuracy: 0.9105\n",
      "Epoch 136/200\n",
      "164/164 - 0s - loss: 0.2042 - accuracy: 0.9113\n",
      "Epoch 137/200\n",
      "164/164 - 0s - loss: 0.2182 - accuracy: 0.8964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138/200\n",
      "164/164 - 0s - loss: 0.2024 - accuracy: 0.9107\n",
      "Epoch 139/200\n",
      "164/164 - 0s - loss: 0.2006 - accuracy: 0.9125\n",
      "Epoch 140/200\n",
      "164/164 - 0s - loss: 0.1957 - accuracy: 0.9178\n",
      "Epoch 141/200\n",
      "164/164 - 0s - loss: 0.2030 - accuracy: 0.9132\n",
      "Epoch 142/200\n",
      "164/164 - 0s - loss: 0.1998 - accuracy: 0.9140\n",
      "Epoch 143/200\n",
      "164/164 - 0s - loss: 0.2002 - accuracy: 0.9155\n",
      "Epoch 144/200\n",
      "164/164 - 0s - loss: 0.2006 - accuracy: 0.9125\n",
      "Epoch 145/200\n",
      "164/164 - 0s - loss: 0.1984 - accuracy: 0.9115\n",
      "Epoch 146/200\n",
      "164/164 - 0s - loss: 0.1948 - accuracy: 0.9197\n",
      "Epoch 147/200\n",
      "164/164 - 0s - loss: 0.1990 - accuracy: 0.9147\n",
      "Epoch 148/200\n",
      "164/164 - 0s - loss: 0.1898 - accuracy: 0.9193\n",
      "Epoch 149/200\n",
      "164/164 - 0s - loss: 0.1987 - accuracy: 0.9115\n",
      "Epoch 150/200\n",
      "164/164 - 0s - loss: 0.1965 - accuracy: 0.9136\n",
      "Epoch 151/200\n",
      "164/164 - 0s - loss: 0.1971 - accuracy: 0.9134\n",
      "Epoch 152/200\n",
      "164/164 - 0s - loss: 0.1921 - accuracy: 0.9182\n",
      "Epoch 153/200\n",
      "164/164 - 0s - loss: 0.1957 - accuracy: 0.9151\n",
      "Epoch 154/200\n",
      "164/164 - 0s - loss: 0.1888 - accuracy: 0.9176\n",
      "Epoch 155/200\n",
      "164/164 - 0s - loss: 0.1949 - accuracy: 0.9168\n",
      "Epoch 156/200\n",
      "164/164 - 0s - loss: 0.1885 - accuracy: 0.9210\n",
      "Epoch 157/200\n",
      "164/164 - 0s - loss: 0.1855 - accuracy: 0.9193\n",
      "Epoch 158/200\n",
      "164/164 - 0s - loss: 0.1838 - accuracy: 0.9201\n",
      "Epoch 159/200\n",
      "164/164 - 0s - loss: 0.1888 - accuracy: 0.9182\n",
      "Epoch 160/200\n",
      "164/164 - 0s - loss: 0.1900 - accuracy: 0.9151\n",
      "Epoch 161/200\n",
      "164/164 - 0s - loss: 0.1908 - accuracy: 0.9172\n",
      "Epoch 162/200\n",
      "164/164 - 0s - loss: 0.1897 - accuracy: 0.9165\n",
      "Epoch 163/200\n",
      "164/164 - 0s - loss: 0.1833 - accuracy: 0.9233\n",
      "Epoch 164/200\n",
      "164/164 - 0s - loss: 0.1905 - accuracy: 0.9189\n",
      "Epoch 165/200\n",
      "164/164 - 0s - loss: 0.1946 - accuracy: 0.9149\n",
      "Epoch 166/200\n",
      "164/164 - 0s - loss: 0.1815 - accuracy: 0.9218\n",
      "Epoch 167/200\n",
      "164/164 - 0s - loss: 0.1839 - accuracy: 0.9237\n",
      "Epoch 168/200\n",
      "164/164 - 0s - loss: 0.1817 - accuracy: 0.9224\n",
      "Epoch 169/200\n",
      "164/164 - 0s - loss: 0.1807 - accuracy: 0.9226\n",
      "Epoch 170/200\n",
      "164/164 - 0s - loss: 0.1851 - accuracy: 0.9210\n",
      "Epoch 171/200\n",
      "164/164 - 0s - loss: 0.1808 - accuracy: 0.9256\n",
      "Epoch 172/200\n",
      "164/164 - 0s - loss: 0.1814 - accuracy: 0.9220\n",
      "Epoch 173/200\n",
      "164/164 - 0s - loss: 0.1792 - accuracy: 0.9218\n",
      "Epoch 174/200\n",
      "164/164 - 0s - loss: 0.1803 - accuracy: 0.9195\n",
      "Epoch 175/200\n",
      "164/164 - 0s - loss: 0.1820 - accuracy: 0.9208\n",
      "Epoch 176/200\n",
      "164/164 - 0s - loss: 0.1853 - accuracy: 0.9174\n",
      "Epoch 177/200\n",
      "164/164 - 0s - loss: 0.1849 - accuracy: 0.9199\n",
      "Epoch 178/200\n",
      "164/164 - 0s - loss: 0.1806 - accuracy: 0.9207\n",
      "Epoch 179/200\n",
      "164/164 - 0s - loss: 0.1781 - accuracy: 0.9201\n",
      "Epoch 180/200\n",
      "164/164 - 0s - loss: 0.1719 - accuracy: 0.9250\n",
      "Epoch 181/200\n",
      "164/164 - 0s - loss: 0.1761 - accuracy: 0.9258\n",
      "Epoch 182/200\n",
      "164/164 - 0s - loss: 0.1760 - accuracy: 0.9239\n",
      "Epoch 183/200\n",
      "164/164 - 0s - loss: 0.1818 - accuracy: 0.9189\n",
      "Epoch 184/200\n",
      "164/164 - 0s - loss: 0.1752 - accuracy: 0.9254\n",
      "Epoch 185/200\n",
      "164/164 - 0s - loss: 0.1832 - accuracy: 0.9193\n",
      "Epoch 186/200\n",
      "164/164 - 0s - loss: 0.1746 - accuracy: 0.9207\n",
      "Epoch 187/200\n",
      "164/164 - 0s - loss: 0.1727 - accuracy: 0.9239\n",
      "Epoch 188/200\n",
      "164/164 - 0s - loss: 0.1880 - accuracy: 0.9182\n",
      "Epoch 189/200\n",
      "164/164 - 0s - loss: 0.1733 - accuracy: 0.9229\n",
      "Epoch 190/200\n",
      "164/164 - 0s - loss: 0.1741 - accuracy: 0.9262\n",
      "Epoch 191/200\n",
      "164/164 - 0s - loss: 0.1733 - accuracy: 0.9250\n",
      "Epoch 192/200\n",
      "164/164 - 0s - loss: 0.1697 - accuracy: 0.9260\n",
      "Epoch 193/200\n",
      "164/164 - 0s - loss: 0.1816 - accuracy: 0.9245\n",
      "Epoch 194/200\n",
      "164/164 - 0s - loss: 0.1756 - accuracy: 0.9258\n",
      "Epoch 195/200\n",
      "164/164 - 0s - loss: 0.1765 - accuracy: 0.9214\n",
      "Epoch 196/200\n",
      "164/164 - 0s - loss: 0.1864 - accuracy: 0.9142\n",
      "Epoch 197/200\n",
      "164/164 - 0s - loss: 0.1737 - accuracy: 0.9239\n",
      "Epoch 198/200\n",
      "164/164 - 0s - loss: 0.1699 - accuracy: 0.9304\n",
      "Epoch 199/200\n",
      "164/164 - 0s - loss: 0.1720 - accuracy: 0.9270\n",
      "Epoch 200/200\n",
      "164/164 - 0s - loss: 0.1718 - accuracy: 0.9237\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x192d8cbb438>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=200,\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantify Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 - 0s - loss: 0.3958 - accuracy: 0.8913\n",
      "Normal Neural Network - Loss: 0.39575839042663574, Accuracy: 0.8913043737411499\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_accuracy = model.evaluate(X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "     CONFIRMED       0.79      0.78      0.79       411\n",
      "FALSE POSITIVE       0.82      0.81      0.82       484\n",
      "     CANDIDATE       0.98      0.99      0.98       853\n",
      "\n",
      "      accuracy                           0.89      1748\n",
      "     macro avg       0.86      0.86      0.86      1748\n",
      "  weighted avg       0.89      0.89      0.89      1748\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate classification report\n",
    "from sklearn.metrics import classification_report\n",
    "predictions = np.argmax(model.predict(X_test_scaled), axis=-1)\n",
    "print(classification_report(encoded_y_test, predictions,\n",
    "                            target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 540x540 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a confusion matrix to visualise the performance \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "predictions = np.argmax(model.predict(X_test_scaled), axis=-1)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true=encoded_y_test, y_pred=predictions)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
    "\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i, s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    "\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "ax.set_xticks([0,1,2])\n",
    "ax.set_xticklabels(target_names)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "ax.set_yticks([0,1,2])\n",
    "ax.set_yticklabels(target_names)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['koi_fpflag_nt', 'koi_fpflag_ss', 'koi_fpflag_co', 'koi_fpflag_ec',\n",
       "       'koi_time0bk', 'koi_impact', 'koi_depth', 'koi_prad', 'koi_teq',\n",
       "       'koi_insol', 'koi_model_snr', 'koi_steff', 'koi_slogg', 'dec',\n",
       "       'koi_kepmag'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use recursive feature elimination to identify the best performing features\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Initiate RFE cross-validation and fit with a linear regression model\n",
    "rfecv = RFECV(\n",
    "    estimator=LinearRegression(),\n",
    "    min_features_to_select=5,\n",
    "    step=5,\n",
    "    n_jobs=-1,\n",
    "    scoring=\"r2\",\n",
    "    cv=5,\n",
    ")\n",
    "\n",
    "_ = rfecv.fit(X_train_scaled, encoded_y_train)\n",
    "\n",
    "# Print the best columns\n",
    "X_train.columns[rfecv.support_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-select the features based on the RFS\n",
    "feature_names = ['koi_fpflag_nt', 'koi_fpflag_ss', 'koi_fpflag_co', 'koi_fpflag_ec',\n",
    "                 'koi_time0bk', 'koi_impact', 'koi_depth', 'koi_prad', 'koi_teq',\n",
    "                 'koi_insol', 'koi_model_snr', 'koi_steff', 'koi_slogg', 'dec',\n",
    "                 'koi_kepmag']\n",
    "\n",
    "X_train = X_train[feature_names]\n",
    "X_test = X_test[feature_names]\n",
    "X_scaler = MinMaxScaler().fit(X_train)\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "164/164 - 0s - loss: 0.5408 - accuracy: 0.7025\n",
      "Epoch 2/200\n",
      "164/164 - 0s - loss: 0.3878 - accuracy: 0.7799\n",
      "Epoch 3/200\n",
      "164/164 - 0s - loss: 0.3827 - accuracy: 0.7765\n",
      "Epoch 4/200\n",
      "164/164 - 0s - loss: 0.3819 - accuracy: 0.7795\n",
      "Epoch 5/200\n",
      "164/164 - 0s - loss: 0.3750 - accuracy: 0.7894\n",
      "Epoch 6/200\n",
      "164/164 - 0s - loss: 0.3742 - accuracy: 0.7858\n",
      "Epoch 7/200\n",
      "164/164 - 0s - loss: 0.3732 - accuracy: 0.7915\n",
      "Epoch 8/200\n",
      "164/164 - 0s - loss: 0.3707 - accuracy: 0.7961\n",
      "Epoch 9/200\n",
      "164/164 - 0s - loss: 0.3675 - accuracy: 0.8007\n",
      "Epoch 10/200\n",
      "164/164 - 0s - loss: 0.3676 - accuracy: 0.8003\n",
      "Epoch 11/200\n",
      "164/164 - 0s - loss: 0.3668 - accuracy: 0.8001\n",
      "Epoch 12/200\n",
      "164/164 - 0s - loss: 0.3685 - accuracy: 0.7984\n",
      "Epoch 13/200\n",
      "164/164 - 0s - loss: 0.3646 - accuracy: 0.8013\n",
      "Epoch 14/200\n",
      "164/164 - 0s - loss: 0.3653 - accuracy: 0.8047\n",
      "Epoch 15/200\n",
      "164/164 - 0s - loss: 0.3623 - accuracy: 0.8106\n",
      "Epoch 16/200\n",
      "164/164 - 0s - loss: 0.3599 - accuracy: 0.8095\n",
      "Epoch 17/200\n",
      "164/164 - 0s - loss: 0.3611 - accuracy: 0.8085\n",
      "Epoch 18/200\n",
      "164/164 - 0s - loss: 0.3575 - accuracy: 0.8074\n",
      "Epoch 19/200\n",
      "164/164 - 0s - loss: 0.3582 - accuracy: 0.8064\n",
      "Epoch 20/200\n",
      "164/164 - 0s - loss: 0.3565 - accuracy: 0.8114\n",
      "Epoch 21/200\n",
      "164/164 - 0s - loss: 0.3545 - accuracy: 0.8152\n",
      "Epoch 22/200\n",
      "164/164 - 0s - loss: 0.3528 - accuracy: 0.8188\n",
      "Epoch 23/200\n",
      "164/164 - 0s - loss: 0.3494 - accuracy: 0.8215\n",
      "Epoch 24/200\n",
      "164/164 - 0s - loss: 0.3497 - accuracy: 0.8200\n",
      "Epoch 25/200\n",
      "164/164 - 0s - loss: 0.3489 - accuracy: 0.8226\n",
      "Epoch 26/200\n",
      "164/164 - 0s - loss: 0.3457 - accuracy: 0.8262\n",
      "Epoch 27/200\n",
      "164/164 - 0s - loss: 0.3467 - accuracy: 0.8253\n",
      "Epoch 28/200\n",
      "164/164 - 0s - loss: 0.3438 - accuracy: 0.8251\n",
      "Epoch 29/200\n",
      "164/164 - 0s - loss: 0.3430 - accuracy: 0.8240\n",
      "Epoch 30/200\n",
      "164/164 - 0s - loss: 0.3439 - accuracy: 0.8228\n",
      "Epoch 31/200\n",
      "164/164 - 0s - loss: 0.3384 - accuracy: 0.8291\n",
      "Epoch 32/200\n",
      "164/164 - 0s - loss: 0.3363 - accuracy: 0.8304\n",
      "Epoch 33/200\n",
      "164/164 - 0s - loss: 0.3368 - accuracy: 0.8243\n",
      "Epoch 34/200\n",
      "164/164 - 0s - loss: 0.3380 - accuracy: 0.8259\n",
      "Epoch 35/200\n",
      "164/164 - 0s - loss: 0.3339 - accuracy: 0.8283\n",
      "Epoch 36/200\n",
      "164/164 - 0s - loss: 0.3321 - accuracy: 0.8346\n",
      "Epoch 37/200\n",
      "164/164 - 0s - loss: 0.3296 - accuracy: 0.8320\n",
      "Epoch 38/200\n",
      "164/164 - 0s - loss: 0.3308 - accuracy: 0.8348\n",
      "Epoch 39/200\n",
      "164/164 - 0s - loss: 0.3273 - accuracy: 0.8346\n",
      "Epoch 40/200\n",
      "164/164 - 0s - loss: 0.3284 - accuracy: 0.8327\n",
      "Epoch 41/200\n",
      "164/164 - 0s - loss: 0.3250 - accuracy: 0.8392\n",
      "Epoch 42/200\n",
      "164/164 - 0s - loss: 0.3240 - accuracy: 0.8358\n",
      "Epoch 43/200\n",
      "164/164 - 0s - loss: 0.3256 - accuracy: 0.8346\n",
      "Epoch 44/200\n",
      "164/164 - 0s - loss: 0.3196 - accuracy: 0.8415\n",
      "Epoch 45/200\n",
      "164/164 - 0s - loss: 0.3190 - accuracy: 0.8419\n",
      "Epoch 46/200\n",
      "164/164 - 0s - loss: 0.3215 - accuracy: 0.8428\n",
      "Epoch 47/200\n",
      "164/164 - 0s - loss: 0.3162 - accuracy: 0.8453\n",
      "Epoch 48/200\n",
      "164/164 - 0s - loss: 0.3159 - accuracy: 0.8484\n",
      "Epoch 49/200\n",
      "164/164 - 0s - loss: 0.3152 - accuracy: 0.8480\n",
      "Epoch 50/200\n",
      "164/164 - 0s - loss: 0.3125 - accuracy: 0.8467\n",
      "Epoch 51/200\n",
      "164/164 - 0s - loss: 0.3135 - accuracy: 0.8419\n",
      "Epoch 52/200\n",
      "164/164 - 0s - loss: 0.3105 - accuracy: 0.8493\n",
      "Epoch 53/200\n",
      "164/164 - 0s - loss: 0.3116 - accuracy: 0.8478\n",
      "Epoch 54/200\n",
      "164/164 - 0s - loss: 0.3081 - accuracy: 0.8486\n",
      "Epoch 55/200\n",
      "164/164 - 0s - loss: 0.3083 - accuracy: 0.8531\n",
      "Epoch 56/200\n",
      "164/164 - 0s - loss: 0.3072 - accuracy: 0.8535\n",
      "Epoch 57/200\n",
      "164/164 - 0s - loss: 0.3090 - accuracy: 0.8497\n",
      "Epoch 58/200\n",
      "164/164 - 0s - loss: 0.3029 - accuracy: 0.8552\n",
      "Epoch 59/200\n",
      "164/164 - 0s - loss: 0.2972 - accuracy: 0.8625\n",
      "Epoch 60/200\n",
      "164/164 - 0s - loss: 0.2987 - accuracy: 0.8606\n",
      "Epoch 61/200\n",
      "164/164 - 0s - loss: 0.2971 - accuracy: 0.8573\n",
      "Epoch 62/200\n",
      "164/164 - 0s - loss: 0.3004 - accuracy: 0.8570\n",
      "Epoch 63/200\n",
      "164/164 - 0s - loss: 0.2966 - accuracy: 0.8604\n",
      "Epoch 64/200\n",
      "164/164 - 0s - loss: 0.2951 - accuracy: 0.8602\n",
      "Epoch 65/200\n",
      "164/164 - 0s - loss: 0.2967 - accuracy: 0.8564\n",
      "Epoch 66/200\n",
      "164/164 - 0s - loss: 0.2945 - accuracy: 0.8600\n",
      "Epoch 67/200\n",
      "164/164 - 0s - loss: 0.2888 - accuracy: 0.8688\n",
      "Epoch 68/200\n",
      "164/164 - 0s - loss: 0.2891 - accuracy: 0.8642\n",
      "Epoch 69/200\n",
      "164/164 - 0s - loss: 0.2884 - accuracy: 0.8631\n",
      "Epoch 70/200\n",
      "164/164 - 0s - loss: 0.2848 - accuracy: 0.8705\n",
      "Epoch 71/200\n",
      "164/164 - 0s - loss: 0.2877 - accuracy: 0.8652\n",
      "Epoch 72/200\n",
      "164/164 - 0s - loss: 0.2867 - accuracy: 0.8604\n",
      "Epoch 73/200\n",
      "164/164 - 0s - loss: 0.2837 - accuracy: 0.8627\n",
      "Epoch 74/200\n",
      "164/164 - 0s - loss: 0.2838 - accuracy: 0.8690\n",
      "Epoch 75/200\n",
      "164/164 - 0s - loss: 0.2816 - accuracy: 0.8730\n",
      "Epoch 76/200\n",
      "164/164 - 0s - loss: 0.2757 - accuracy: 0.8783\n",
      "Epoch 77/200\n",
      "164/164 - 0s - loss: 0.2793 - accuracy: 0.8730\n",
      "Epoch 78/200\n",
      "164/164 - 0s - loss: 0.2759 - accuracy: 0.8739\n",
      "Epoch 79/200\n",
      "164/164 - 0s - loss: 0.2733 - accuracy: 0.8764\n",
      "Epoch 80/200\n",
      "164/164 - 0s - loss: 0.2825 - accuracy: 0.8713\n",
      "Epoch 81/200\n",
      "164/164 - 0s - loss: 0.2768 - accuracy: 0.8695\n",
      "Epoch 82/200\n",
      "164/164 - 0s - loss: 0.2734 - accuracy: 0.8747\n",
      "Epoch 83/200\n",
      "164/164 - 0s - loss: 0.2737 - accuracy: 0.8739\n",
      "Epoch 84/200\n",
      "164/164 - 0s - loss: 0.2703 - accuracy: 0.8774\n",
      "Epoch 85/200\n",
      "164/164 - 0s - loss: 0.2704 - accuracy: 0.8816\n",
      "Epoch 86/200\n",
      "164/164 - 0s - loss: 0.2691 - accuracy: 0.8768\n",
      "Epoch 87/200\n",
      "164/164 - 0s - loss: 0.2666 - accuracy: 0.8791\n",
      "Epoch 88/200\n",
      "164/164 - 0s - loss: 0.2654 - accuracy: 0.8781\n",
      "Epoch 89/200\n",
      "164/164 - 0s - loss: 0.2678 - accuracy: 0.8793\n",
      "Epoch 90/200\n",
      "164/164 - 0s - loss: 0.2638 - accuracy: 0.8829\n",
      "Epoch 91/200\n",
      "164/164 - 0s - loss: 0.2647 - accuracy: 0.8798\n",
      "Epoch 92/200\n",
      "164/164 - 0s - loss: 0.2645 - accuracy: 0.8787\n",
      "Epoch 93/200\n",
      "164/164 - 0s - loss: 0.2667 - accuracy: 0.8753\n",
      "Epoch 94/200\n",
      "164/164 - 0s - loss: 0.2630 - accuracy: 0.8796\n",
      "Epoch 95/200\n",
      "164/164 - 0s - loss: 0.2596 - accuracy: 0.8806\n",
      "Epoch 96/200\n",
      "164/164 - 0s - loss: 0.2646 - accuracy: 0.8777\n",
      "Epoch 97/200\n",
      "164/164 - 0s - loss: 0.2577 - accuracy: 0.8827\n",
      "Epoch 98/200\n",
      "164/164 - 0s - loss: 0.2570 - accuracy: 0.8819\n",
      "Epoch 99/200\n",
      "164/164 - 0s - loss: 0.2557 - accuracy: 0.8838\n",
      "Epoch 100/200\n",
      "164/164 - 0s - loss: 0.2561 - accuracy: 0.8846\n",
      "Epoch 101/200\n",
      "164/164 - 0s - loss: 0.2571 - accuracy: 0.8840\n",
      "Epoch 102/200\n",
      "164/164 - 0s - loss: 0.2533 - accuracy: 0.8882\n",
      "Epoch 103/200\n",
      "164/164 - 0s - loss: 0.2532 - accuracy: 0.8846\n",
      "Epoch 104/200\n",
      "164/164 - 0s - loss: 0.2492 - accuracy: 0.8909\n",
      "Epoch 105/200\n",
      "164/164 - 0s - loss: 0.2550 - accuracy: 0.8838\n",
      "Epoch 106/200\n",
      "164/164 - 0s - loss: 0.2494 - accuracy: 0.8871\n",
      "Epoch 107/200\n",
      "164/164 - 0s - loss: 0.2505 - accuracy: 0.8886\n",
      "Epoch 108/200\n",
      "164/164 - 0s - loss: 0.2476 - accuracy: 0.8880\n",
      "Epoch 109/200\n",
      "164/164 - 0s - loss: 0.2533 - accuracy: 0.8823\n",
      "Epoch 110/200\n",
      "164/164 - 0s - loss: 0.2523 - accuracy: 0.8875\n",
      "Epoch 111/200\n",
      "164/164 - 0s - loss: 0.2475 - accuracy: 0.8875\n",
      "Epoch 112/200\n",
      "164/164 - 0s - loss: 0.2471 - accuracy: 0.8890\n",
      "Epoch 113/200\n",
      "164/164 - 0s - loss: 0.2591 - accuracy: 0.8774\n",
      "Epoch 114/200\n",
      "164/164 - 0s - loss: 0.2538 - accuracy: 0.8804\n",
      "Epoch 115/200\n",
      "164/164 - 0s - loss: 0.2443 - accuracy: 0.8936\n",
      "Epoch 116/200\n",
      "164/164 - 0s - loss: 0.2453 - accuracy: 0.8880\n",
      "Epoch 117/200\n",
      "164/164 - 0s - loss: 0.2427 - accuracy: 0.8911\n",
      "Epoch 118/200\n",
      "164/164 - 0s - loss: 0.2429 - accuracy: 0.8917\n",
      "Epoch 119/200\n",
      "164/164 - 0s - loss: 0.2516 - accuracy: 0.8869\n",
      "Epoch 120/200\n",
      "164/164 - 0s - loss: 0.2453 - accuracy: 0.8869\n",
      "Epoch 121/200\n",
      "164/164 - 0s - loss: 0.2413 - accuracy: 0.8880\n",
      "Epoch 122/200\n",
      "164/164 - 0s - loss: 0.2409 - accuracy: 0.8882\n",
      "Epoch 123/200\n",
      "164/164 - 0s - loss: 0.2426 - accuracy: 0.8899\n",
      "Epoch 124/200\n",
      "164/164 - 0s - loss: 0.2484 - accuracy: 0.8831\n",
      "Epoch 125/200\n",
      "164/164 - 0s - loss: 0.2405 - accuracy: 0.8877\n",
      "Epoch 126/200\n",
      "164/164 - 0s - loss: 0.2480 - accuracy: 0.8785\n",
      "Epoch 127/200\n",
      "164/164 - 0s - loss: 0.2431 - accuracy: 0.8880\n",
      "Epoch 128/200\n",
      "164/164 - 0s - loss: 0.2510 - accuracy: 0.8833\n",
      "Epoch 129/200\n",
      "164/164 - 0s - loss: 0.2345 - accuracy: 0.8974\n",
      "Epoch 130/200\n",
      "164/164 - 0s - loss: 0.2406 - accuracy: 0.8869\n",
      "Epoch 131/200\n",
      "164/164 - 0s - loss: 0.2427 - accuracy: 0.8909\n",
      "Epoch 132/200\n",
      "164/164 - 0s - loss: 0.2455 - accuracy: 0.8854\n",
      "Epoch 133/200\n",
      "164/164 - 0s - loss: 0.2340 - accuracy: 0.8936\n",
      "Epoch 134/200\n",
      "164/164 - 0s - loss: 0.2362 - accuracy: 0.8901\n",
      "Epoch 135/200\n",
      "164/164 - 0s - loss: 0.2350 - accuracy: 0.8953\n",
      "Epoch 136/200\n",
      "164/164 - 0s - loss: 0.2376 - accuracy: 0.8941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/200\n",
      "164/164 - 0s - loss: 0.2314 - accuracy: 0.8951\n",
      "Epoch 138/200\n",
      "164/164 - 0s - loss: 0.2378 - accuracy: 0.8913\n",
      "Epoch 139/200\n",
      "164/164 - 0s - loss: 0.2364 - accuracy: 0.8911\n",
      "Epoch 140/200\n",
      "164/164 - 0s - loss: 0.2354 - accuracy: 0.8926\n",
      "Epoch 141/200\n",
      "164/164 - 0s - loss: 0.2330 - accuracy: 0.8945\n",
      "Epoch 142/200\n",
      "164/164 - 0s - loss: 0.2322 - accuracy: 0.8949\n",
      "Epoch 143/200\n",
      "164/164 - 0s - loss: 0.2339 - accuracy: 0.8962\n",
      "Epoch 144/200\n",
      "164/164 - 0s - loss: 0.2300 - accuracy: 0.8943\n",
      "Epoch 145/200\n",
      "164/164 - 0s - loss: 0.2358 - accuracy: 0.8911\n",
      "Epoch 146/200\n",
      "164/164 - 0s - loss: 0.2324 - accuracy: 0.8966\n",
      "Epoch 147/200\n",
      "164/164 - 0s - loss: 0.2328 - accuracy: 0.8940\n",
      "Epoch 148/200\n",
      "164/164 - 0s - loss: 0.2312 - accuracy: 0.8964\n",
      "Epoch 149/200\n",
      "164/164 - 0s - loss: 0.2309 - accuracy: 0.8947\n",
      "Epoch 150/200\n",
      "164/164 - 0s - loss: 0.2280 - accuracy: 0.8989\n",
      "Epoch 151/200\n",
      "164/164 - 0s - loss: 0.2291 - accuracy: 0.8968\n",
      "Epoch 152/200\n",
      "164/164 - 0s - loss: 0.2245 - accuracy: 0.9004\n",
      "Epoch 153/200\n",
      "164/164 - 0s - loss: 0.2459 - accuracy: 0.8863\n",
      "Epoch 154/200\n",
      "164/164 - 0s - loss: 0.2290 - accuracy: 0.8938\n",
      "Epoch 155/200\n",
      "164/164 - 0s - loss: 0.2313 - accuracy: 0.8953\n",
      "Epoch 156/200\n",
      "164/164 - 0s - loss: 0.2271 - accuracy: 0.8995\n",
      "Epoch 157/200\n",
      "164/164 - 0s - loss: 0.2360 - accuracy: 0.8907\n",
      "Epoch 158/200\n",
      "164/164 - 0s - loss: 0.2290 - accuracy: 0.8970\n",
      "Epoch 159/200\n",
      "164/164 - 0s - loss: 0.2244 - accuracy: 0.8981\n",
      "Epoch 160/200\n",
      "164/164 - 0s - loss: 0.2308 - accuracy: 0.8938\n",
      "Epoch 161/200\n",
      "164/164 - 0s - loss: 0.2313 - accuracy: 0.8951\n",
      "Epoch 162/200\n",
      "164/164 - 0s - loss: 0.2344 - accuracy: 0.8924\n",
      "Epoch 163/200\n",
      "164/164 - 0s - loss: 0.2303 - accuracy: 0.8947\n",
      "Epoch 164/200\n",
      "164/164 - 0s - loss: 0.2262 - accuracy: 0.9002\n",
      "Epoch 165/200\n",
      "164/164 - 0s - loss: 0.2287 - accuracy: 0.8985\n",
      "Epoch 166/200\n",
      "164/164 - 0s - loss: 0.2218 - accuracy: 0.8993\n",
      "Epoch 167/200\n",
      "164/164 - 0s - loss: 0.2266 - accuracy: 0.8964\n",
      "Epoch 168/200\n",
      "164/164 - 0s - loss: 0.2301 - accuracy: 0.8940\n",
      "Epoch 169/200\n",
      "164/164 - 0s - loss: 0.2251 - accuracy: 0.8953\n",
      "Epoch 170/200\n",
      "164/164 - 0s - loss: 0.2217 - accuracy: 0.8999\n",
      "Epoch 171/200\n",
      "164/164 - 0s - loss: 0.2311 - accuracy: 0.8951\n",
      "Epoch 172/200\n",
      "164/164 - 0s - loss: 0.2301 - accuracy: 0.8943\n",
      "Epoch 173/200\n",
      "164/164 - 0s - loss: 0.2197 - accuracy: 0.9010\n",
      "Epoch 174/200\n",
      "164/164 - 0s - loss: 0.2260 - accuracy: 0.8987\n",
      "Epoch 175/200\n",
      "164/164 - 0s - loss: 0.2239 - accuracy: 0.8968\n",
      "Epoch 176/200\n",
      "164/164 - 0s - loss: 0.2282 - accuracy: 0.8949\n",
      "Epoch 177/200\n",
      "164/164 - 0s - loss: 0.2243 - accuracy: 0.8991\n",
      "Epoch 178/200\n",
      "164/164 - 0s - loss: 0.2221 - accuracy: 0.8972\n",
      "Epoch 179/200\n",
      "164/164 - 0s - loss: 0.2283 - accuracy: 0.8987\n",
      "Epoch 180/200\n",
      "164/164 - 0s - loss: 0.2211 - accuracy: 0.8976\n",
      "Epoch 181/200\n",
      "164/164 - 0s - loss: 0.2202 - accuracy: 0.9008\n",
      "Epoch 182/200\n",
      "164/164 - 0s - loss: 0.2201 - accuracy: 0.8995\n",
      "Epoch 183/200\n",
      "164/164 - 0s - loss: 0.2220 - accuracy: 0.8983\n",
      "Epoch 184/200\n",
      "164/164 - 0s - loss: 0.2176 - accuracy: 0.9029\n",
      "Epoch 185/200\n",
      "164/164 - 0s - loss: 0.2183 - accuracy: 0.9012\n",
      "Epoch 186/200\n",
      "164/164 - 0s - loss: 0.2171 - accuracy: 0.9035\n",
      "Epoch 187/200\n",
      "164/164 - 0s - loss: 0.2169 - accuracy: 0.9016\n",
      "Epoch 188/200\n",
      "164/164 - 0s - loss: 0.2178 - accuracy: 0.9002\n",
      "Epoch 189/200\n",
      "164/164 - 0s - loss: 0.2189 - accuracy: 0.9004\n",
      "Epoch 190/200\n",
      "164/164 - 0s - loss: 0.2143 - accuracy: 0.9039\n",
      "Epoch 191/200\n",
      "164/164 - 0s - loss: 0.2181 - accuracy: 0.9029\n",
      "Epoch 192/200\n",
      "164/164 - 0s - loss: 0.2188 - accuracy: 0.8976\n",
      "Epoch 193/200\n",
      "164/164 - 0s - loss: 0.2177 - accuracy: 0.9001\n",
      "Epoch 194/200\n",
      "164/164 - 0s - loss: 0.2210 - accuracy: 0.9001\n",
      "Epoch 195/200\n",
      "164/164 - 0s - loss: 0.2178 - accuracy: 0.9006\n",
      "Epoch 196/200\n",
      "164/164 - 0s - loss: 0.2173 - accuracy: 0.9020\n",
      "Epoch 197/200\n",
      "164/164 - 0s - loss: 0.2202 - accuracy: 0.9002\n",
      "Epoch 198/200\n",
      "164/164 - 0s - loss: 0.2184 - accuracy: 0.9029\n",
      "Epoch 199/200\n",
      "164/164 - 0s - loss: 0.2174 - accuracy: 0.9006\n",
      "Epoch 200/200\n",
      "164/164 - 0s - loss: 0.2139 - accuracy: 0.9033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x193021cb6d8>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Re-run the model with the new-feature selection\n",
    "inputs = X_train.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=100, activation='relu', input_dim=inputs))\n",
    "model.add(Dense(units=100, activation='relu'))\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=200,\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 - 0s - loss: 0.3301 - accuracy: 0.8924\n",
      "Normal Neural Network - Loss: 0.3301277756690979, Accuracy: 0.8924484848976135\n"
     ]
    }
   ],
   "source": [
    "model_loss, model_accuracy = model.evaluate(X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 257 Complete [00h 00m 01s]\n",
      "val_accuracy: 0.769304096698761\n",
      "\n",
      "Best val_accuracy So Far: 0.8045758008956909\n",
      "Total elapsed time: 00h 04m 35s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "\n",
    "def build_model(hp):\n",
    "    model = tensorflow.keras.Sequential()\n",
    "\n",
    "    # Tune the number of units in the layers\n",
    "    hp_units1 = hp.Int('units1', min_value=50, max_value=400, step=50)\n",
    "    hp_units2 = hp.Int('units2', min_value=50, max_value=400, step=50)\n",
    "        \n",
    "    # Add the layers      \n",
    "    model.add(Dense(units=hp_units1, activation='relu', input_dim=inputs))\n",
    "    model.add(Dense(units=hp_units2, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    # Tune the learning rate for the optimizer\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[0.001, 0.01, 0.1, 0.2, 0.3])\n",
    "\n",
    "    model.compile(\n",
    "      optimizer = tensorflow.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "      loss = 'categorical_crossentropy',\n",
    "      metrics = ['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# HyperBand algorithm from keras_tuner\n",
    "tuner = kt.Hyperband(\n",
    "        build_model,\n",
    "        objective='val_accuracy',\n",
    "        max_epochs=1000,\n",
    "        directory='hp_dir',\n",
    "        project_name='neural_net_tuning'\n",
    "        )\n",
    "\n",
    "# Perform the hyperperameter tuning \n",
    "tuner.search(X_train_scaled, y_train_categorical, epochs=200, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 0s 720us/step - loss: 0.4021 - accuracy: 0.7706\n"
     ]
    }
   ],
   "source": [
    "# Extract the best model and evaluate with the test data\n",
    "# Print accuracy and loss with test data\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "loss, accuracy = best_model.evaluate(X_test_scaled, y_test_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "units1 300\n",
      "units2 350\n",
      "learning_rate 0.001\n"
     ]
    }
   ],
   "source": [
    "# Print optimal hyperparamaters\n",
    "for h_param in [f\"units{i}\" for i in range(1,3)] + ['learning_rate']:\n",
    "    print(h_param, tuner.get_best_hyperparameters()[0].get(h_param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the adjusted model\n",
    "inputs = X_train.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=300, activation='relu', input_dim=inputs))\n",
    "model.add(Dense(units=350, activation='relu'))\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "# Compile the adjusted model\n",
    "model.compile(optimizer=tensorflow.keras.optimizers.Adam(learning_rate=0.01),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "164/164 - 0s - loss: 0.4230 - accuracy: 0.7498\n",
      "Epoch 2/200\n",
      "164/164 - 0s - loss: 0.3944 - accuracy: 0.7658\n",
      "Epoch 3/200\n",
      "164/164 - 0s - loss: 0.3896 - accuracy: 0.7744\n",
      "Epoch 4/200\n",
      "164/164 - 0s - loss: 0.3917 - accuracy: 0.7721\n",
      "Epoch 5/200\n",
      "164/164 - 0s - loss: 0.3931 - accuracy: 0.7612\n",
      "Epoch 6/200\n",
      "164/164 - 0s - loss: 0.3837 - accuracy: 0.7793\n",
      "Epoch 7/200\n",
      "164/164 - 0s - loss: 0.3812 - accuracy: 0.7793\n",
      "Epoch 8/200\n",
      "164/164 - 0s - loss: 0.3756 - accuracy: 0.7894\n",
      "Epoch 9/200\n",
      "164/164 - 0s - loss: 0.3732 - accuracy: 0.7953\n",
      "Epoch 10/200\n",
      "164/164 - 0s - loss: 0.3751 - accuracy: 0.7892\n",
      "Epoch 11/200\n",
      "164/164 - 0s - loss: 0.3765 - accuracy: 0.7793\n",
      "Epoch 12/200\n",
      "164/164 - 0s - loss: 0.3741 - accuracy: 0.7919\n",
      "Epoch 13/200\n",
      "164/164 - 0s - loss: 0.3663 - accuracy: 0.8009\n",
      "Epoch 14/200\n",
      "164/164 - 0s - loss: 0.3569 - accuracy: 0.8129\n",
      "Epoch 15/200\n",
      "164/164 - 0s - loss: 0.3525 - accuracy: 0.8179\n",
      "Epoch 16/200\n",
      "164/164 - 0s - loss: 0.3548 - accuracy: 0.8125\n",
      "Epoch 17/200\n",
      "164/164 - 0s - loss: 0.3469 - accuracy: 0.8278\n",
      "Epoch 18/200\n",
      "164/164 - 0s - loss: 0.3381 - accuracy: 0.8331\n",
      "Epoch 19/200\n",
      "164/164 - 0s - loss: 0.3432 - accuracy: 0.8257\n",
      "Epoch 20/200\n",
      "164/164 - 0s - loss: 0.3392 - accuracy: 0.8369\n",
      "Epoch 21/200\n",
      "164/164 - 0s - loss: 0.3320 - accuracy: 0.8425\n",
      "Epoch 22/200\n",
      "164/164 - 0s - loss: 0.3287 - accuracy: 0.8419\n",
      "Epoch 23/200\n",
      "164/164 - 0s - loss: 0.3396 - accuracy: 0.8285\n",
      "Epoch 24/200\n",
      "164/164 - 0s - loss: 0.3249 - accuracy: 0.8474\n",
      "Epoch 25/200\n",
      "164/164 - 0s - loss: 0.3253 - accuracy: 0.8423\n",
      "Epoch 26/200\n",
      "164/164 - 0s - loss: 0.3266 - accuracy: 0.8472\n",
      "Epoch 27/200\n",
      "164/164 - 0s - loss: 0.3193 - accuracy: 0.8524\n",
      "Epoch 28/200\n",
      "164/164 - 0s - loss: 0.3186 - accuracy: 0.8524\n",
      "Epoch 29/200\n",
      "164/164 - 0s - loss: 0.3128 - accuracy: 0.8541\n",
      "Epoch 30/200\n",
      "164/164 - 0s - loss: 0.3175 - accuracy: 0.8522\n",
      "Epoch 31/200\n",
      "164/164 - 0s - loss: 0.3115 - accuracy: 0.8526\n",
      "Epoch 32/200\n",
      "164/164 - 0s - loss: 0.3145 - accuracy: 0.8549\n",
      "Epoch 33/200\n",
      "164/164 - 0s - loss: 0.3227 - accuracy: 0.8474\n",
      "Epoch 34/200\n",
      "164/164 - 0s - loss: 0.3073 - accuracy: 0.8596\n",
      "Epoch 35/200\n",
      "164/164 - 0s - loss: 0.3429 - accuracy: 0.8610\n",
      "Epoch 36/200\n",
      "164/164 - 0s - loss: 0.3162 - accuracy: 0.8552\n",
      "Epoch 37/200\n",
      "164/164 - 0s - loss: 0.2988 - accuracy: 0.8652\n",
      "Epoch 38/200\n",
      "164/164 - 0s - loss: 0.2972 - accuracy: 0.8632\n",
      "Epoch 39/200\n",
      "164/164 - 0s - loss: 0.2957 - accuracy: 0.8678\n",
      "Epoch 40/200\n",
      "164/164 - 0s - loss: 0.2929 - accuracy: 0.8709\n",
      "Epoch 41/200\n",
      "164/164 - 0s - loss: 0.2886 - accuracy: 0.8688\n",
      "Epoch 42/200\n",
      "164/164 - 0s - loss: 0.2940 - accuracy: 0.8642\n",
      "Epoch 43/200\n",
      "164/164 - 0s - loss: 0.2888 - accuracy: 0.8695\n",
      "Epoch 44/200\n",
      "164/164 - 0s - loss: 0.3074 - accuracy: 0.8604\n",
      "Epoch 45/200\n",
      "164/164 - 0s - loss: 0.2962 - accuracy: 0.8707\n",
      "Epoch 46/200\n",
      "164/164 - 0s - loss: 0.2868 - accuracy: 0.8734\n",
      "Epoch 47/200\n",
      "164/164 - 0s - loss: 0.2979 - accuracy: 0.8610\n",
      "Epoch 48/200\n",
      "164/164 - 0s - loss: 0.2895 - accuracy: 0.8741\n",
      "Epoch 49/200\n",
      "164/164 - 0s - loss: 0.2971 - accuracy: 0.8632\n",
      "Epoch 50/200\n",
      "164/164 - 0s - loss: 0.2860 - accuracy: 0.8755\n",
      "Epoch 51/200\n",
      "164/164 - 0s - loss: 0.2948 - accuracy: 0.8659\n",
      "Epoch 52/200\n",
      "164/164 - 0s - loss: 0.2884 - accuracy: 0.8739\n",
      "Epoch 53/200\n",
      "164/164 - 0s - loss: 0.2941 - accuracy: 0.8671\n",
      "Epoch 54/200\n",
      "164/164 - 0s - loss: 0.2856 - accuracy: 0.8741\n",
      "Epoch 55/200\n",
      "164/164 - 0s - loss: 0.2927 - accuracy: 0.8724\n",
      "Epoch 56/200\n",
      "164/164 - 0s - loss: 0.2971 - accuracy: 0.8636\n",
      "Epoch 57/200\n",
      "164/164 - 0s - loss: 0.2963 - accuracy: 0.8665\n",
      "Epoch 58/200\n",
      "164/164 - 0s - loss: 0.2880 - accuracy: 0.8716\n",
      "Epoch 59/200\n",
      "164/164 - 0s - loss: 0.2867 - accuracy: 0.8682\n",
      "Epoch 60/200\n",
      "164/164 - 0s - loss: 0.2856 - accuracy: 0.8705\n",
      "Epoch 61/200\n",
      "164/164 - 0s - loss: 0.2765 - accuracy: 0.8783\n",
      "Epoch 62/200\n",
      "164/164 - 0s - loss: 0.2969 - accuracy: 0.8663\n",
      "Epoch 63/200\n",
      "164/164 - 0s - loss: 0.2935 - accuracy: 0.8669\n",
      "Epoch 64/200\n",
      "164/164 - 0s - loss: 0.2825 - accuracy: 0.8745\n",
      "Epoch 65/200\n",
      "164/164 - 0s - loss: 0.2820 - accuracy: 0.8722\n",
      "Epoch 66/200\n",
      "164/164 - 0s - loss: 0.2837 - accuracy: 0.8730\n",
      "Epoch 67/200\n",
      "164/164 - 0s - loss: 0.2767 - accuracy: 0.8795\n",
      "Epoch 68/200\n",
      "164/164 - 0s - loss: 0.2817 - accuracy: 0.8735\n",
      "Epoch 69/200\n",
      "164/164 - 0s - loss: 0.2787 - accuracy: 0.8751\n",
      "Epoch 70/200\n",
      "164/164 - 0s - loss: 0.2870 - accuracy: 0.8688\n",
      "Epoch 71/200\n",
      "164/164 - 0s - loss: 0.2849 - accuracy: 0.8730\n",
      "Epoch 72/200\n",
      "164/164 - 0s - loss: 0.2783 - accuracy: 0.8779\n",
      "Epoch 73/200\n",
      "164/164 - 0s - loss: 0.2765 - accuracy: 0.8762\n",
      "Epoch 74/200\n",
      "164/164 - 0s - loss: 0.2810 - accuracy: 0.8753\n",
      "Epoch 75/200\n",
      "164/164 - 0s - loss: 0.2806 - accuracy: 0.8741\n",
      "Epoch 76/200\n",
      "164/164 - 0s - loss: 0.2765 - accuracy: 0.8749\n",
      "Epoch 77/200\n",
      "164/164 - 0s - loss: 0.2705 - accuracy: 0.8819\n",
      "Epoch 78/200\n",
      "164/164 - 0s - loss: 0.2794 - accuracy: 0.8749\n",
      "Epoch 79/200\n",
      "164/164 - 0s - loss: 0.2981 - accuracy: 0.8587\n",
      "Epoch 80/200\n",
      "164/164 - 0s - loss: 0.2792 - accuracy: 0.8814\n",
      "Epoch 81/200\n",
      "164/164 - 0s - loss: 0.2802 - accuracy: 0.8722\n",
      "Epoch 82/200\n",
      "164/164 - 0s - loss: 0.2785 - accuracy: 0.8695\n",
      "Epoch 83/200\n",
      "164/164 - 0s - loss: 0.2747 - accuracy: 0.8741\n",
      "Epoch 84/200\n",
      "164/164 - 0s - loss: 0.2837 - accuracy: 0.8743\n",
      "Epoch 85/200\n",
      "164/164 - 0s - loss: 0.2744 - accuracy: 0.8779\n",
      "Epoch 86/200\n",
      "164/164 - 0s - loss: 0.2757 - accuracy: 0.8772\n",
      "Epoch 87/200\n",
      "164/164 - 0s - loss: 0.2770 - accuracy: 0.8766\n",
      "Epoch 88/200\n",
      "164/164 - 0s - loss: 0.2795 - accuracy: 0.8714\n",
      "Epoch 89/200\n",
      "164/164 - 0s - loss: 0.2824 - accuracy: 0.8705\n",
      "Epoch 90/200\n",
      "164/164 - 0s - loss: 0.2736 - accuracy: 0.8734\n",
      "Epoch 91/200\n",
      "164/164 - 0s - loss: 0.2691 - accuracy: 0.8819\n",
      "Epoch 92/200\n",
      "164/164 - 0s - loss: 0.2707 - accuracy: 0.8804\n",
      "Epoch 93/200\n",
      "164/164 - 0s - loss: 0.2748 - accuracy: 0.8789\n",
      "Epoch 94/200\n",
      "164/164 - 0s - loss: 0.2683 - accuracy: 0.8812\n",
      "Epoch 95/200\n",
      "164/164 - 0s - loss: 0.2679 - accuracy: 0.8829\n",
      "Epoch 96/200\n",
      "164/164 - 0s - loss: 0.2675 - accuracy: 0.8819\n",
      "Epoch 97/200\n",
      "164/164 - 0s - loss: 0.2805 - accuracy: 0.8735\n",
      "Epoch 98/200\n",
      "164/164 - 0s - loss: 0.2686 - accuracy: 0.8810\n",
      "Epoch 99/200\n",
      "164/164 - 0s - loss: 0.2787 - accuracy: 0.8791\n",
      "Epoch 100/200\n",
      "164/164 - 0s - loss: 0.2730 - accuracy: 0.8753\n",
      "Epoch 101/200\n",
      "164/164 - 0s - loss: 0.2680 - accuracy: 0.8814\n",
      "Epoch 102/200\n",
      "164/164 - 0s - loss: 0.2712 - accuracy: 0.8749\n",
      "Epoch 103/200\n",
      "164/164 - 0s - loss: 0.2672 - accuracy: 0.8802\n",
      "Epoch 104/200\n",
      "164/164 - 0s - loss: 0.2702 - accuracy: 0.8783\n",
      "Epoch 105/200\n",
      "164/164 - 0s - loss: 0.2682 - accuracy: 0.8810\n",
      "Epoch 106/200\n",
      "164/164 - 0s - loss: 0.2710 - accuracy: 0.8825\n",
      "Epoch 107/200\n",
      "164/164 - 0s - loss: 0.2665 - accuracy: 0.8802\n",
      "Epoch 108/200\n",
      "164/164 - 0s - loss: 0.2661 - accuracy: 0.8776\n",
      "Epoch 109/200\n",
      "164/164 - 0s - loss: 0.2630 - accuracy: 0.8825\n",
      "Epoch 110/200\n",
      "164/164 - 0s - loss: 0.2620 - accuracy: 0.8867\n",
      "Epoch 111/200\n",
      "164/164 - 0s - loss: 0.2634 - accuracy: 0.8814\n",
      "Epoch 112/200\n",
      "164/164 - 0s - loss: 0.2686 - accuracy: 0.8789\n",
      "Epoch 113/200\n",
      "164/164 - 0s - loss: 0.2629 - accuracy: 0.8831\n",
      "Epoch 114/200\n",
      "164/164 - 0s - loss: 0.2626 - accuracy: 0.8823\n",
      "Epoch 115/200\n",
      "164/164 - 0s - loss: 0.2608 - accuracy: 0.8837\n",
      "Epoch 116/200\n",
      "164/164 - 0s - loss: 0.2695 - accuracy: 0.8791\n",
      "Epoch 117/200\n",
      "164/164 - 0s - loss: 0.2649 - accuracy: 0.8795\n",
      "Epoch 118/200\n",
      "164/164 - 0s - loss: 0.2664 - accuracy: 0.8774\n",
      "Epoch 119/200\n",
      "164/164 - 0s - loss: 0.2637 - accuracy: 0.8817\n",
      "Epoch 120/200\n",
      "164/164 - 0s - loss: 0.2580 - accuracy: 0.8861\n",
      "Epoch 121/200\n",
      "164/164 - 0s - loss: 0.2696 - accuracy: 0.8764\n",
      "Epoch 122/200\n",
      "164/164 - 0s - loss: 0.3226 - accuracy: 0.8781\n",
      "Epoch 123/200\n",
      "164/164 - 0s - loss: 0.2805 - accuracy: 0.8774\n",
      "Epoch 124/200\n",
      "164/164 - 0s - loss: 0.2637 - accuracy: 0.8840\n",
      "Epoch 125/200\n",
      "164/164 - 0s - loss: 0.2645 - accuracy: 0.8817\n",
      "Epoch 126/200\n",
      "164/164 - 0s - loss: 0.2590 - accuracy: 0.8877\n",
      "Epoch 127/200\n",
      "164/164 - 0s - loss: 0.2547 - accuracy: 0.8850\n",
      "Epoch 128/200\n",
      "164/164 - 0s - loss: 0.2622 - accuracy: 0.8816\n",
      "Epoch 129/200\n",
      "164/164 - 0s - loss: 0.2619 - accuracy: 0.8800\n",
      "Epoch 130/200\n",
      "164/164 - 0s - loss: 0.2593 - accuracy: 0.8850\n",
      "Epoch 131/200\n",
      "164/164 - 0s - loss: 0.2601 - accuracy: 0.8812\n",
      "Epoch 132/200\n",
      "164/164 - 0s - loss: 0.2580 - accuracy: 0.8835\n",
      "Epoch 133/200\n",
      "164/164 - 0s - loss: 0.2653 - accuracy: 0.8833\n",
      "Epoch 134/200\n",
      "164/164 - 0s - loss: 0.2594 - accuracy: 0.8806\n",
      "Epoch 135/200\n",
      "164/164 - 0s - loss: 0.2586 - accuracy: 0.8829\n",
      "Epoch 136/200\n",
      "164/164 - 0s - loss: 0.2571 - accuracy: 0.8808\n",
      "Epoch 137/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164/164 - 0s - loss: 0.2635 - accuracy: 0.8825\n",
      "Epoch 138/200\n",
      "164/164 - 0s - loss: 0.2676 - accuracy: 0.8817\n",
      "Epoch 139/200\n",
      "164/164 - 0s - loss: 0.2758 - accuracy: 0.8785\n",
      "Epoch 140/200\n",
      "164/164 - 0s - loss: 0.2680 - accuracy: 0.8756\n",
      "Epoch 141/200\n",
      "164/164 - 0s - loss: 0.2584 - accuracy: 0.8835\n",
      "Epoch 142/200\n",
      "164/164 - 0s - loss: 0.2574 - accuracy: 0.8867\n",
      "Epoch 143/200\n",
      "164/164 - 0s - loss: 0.2630 - accuracy: 0.8798\n",
      "Epoch 144/200\n",
      "164/164 - 0s - loss: 0.2595 - accuracy: 0.8829\n",
      "Epoch 145/200\n",
      "164/164 - 0s - loss: 0.2614 - accuracy: 0.8831\n",
      "Epoch 146/200\n",
      "164/164 - 0s - loss: 0.2646 - accuracy: 0.8804\n",
      "Epoch 147/200\n",
      "164/164 - 0s - loss: 0.2567 - accuracy: 0.8863\n",
      "Epoch 148/200\n",
      "164/164 - 0s - loss: 0.2583 - accuracy: 0.8837\n",
      "Epoch 149/200\n",
      "164/164 - 0s - loss: 0.2552 - accuracy: 0.8856\n",
      "Epoch 150/200\n",
      "164/164 - 0s - loss: 0.2590 - accuracy: 0.8833\n",
      "Epoch 151/200\n",
      "164/164 - 0s - loss: 0.2576 - accuracy: 0.8863\n",
      "Epoch 152/200\n",
      "164/164 - 0s - loss: 0.2605 - accuracy: 0.8827\n",
      "Epoch 153/200\n",
      "164/164 - 0s - loss: 0.2645 - accuracy: 0.8762\n",
      "Epoch 154/200\n",
      "164/164 - 0s - loss: 0.2538 - accuracy: 0.8854\n",
      "Epoch 155/200\n",
      "164/164 - 0s - loss: 0.2624 - accuracy: 0.8837\n",
      "Epoch 156/200\n",
      "164/164 - 0s - loss: 0.2547 - accuracy: 0.8829\n",
      "Epoch 157/200\n",
      "164/164 - 0s - loss: 0.2600 - accuracy: 0.8854\n",
      "Epoch 158/200\n",
      "164/164 - 0s - loss: 0.2585 - accuracy: 0.8833\n",
      "Epoch 159/200\n",
      "164/164 - 0s - loss: 0.2587 - accuracy: 0.8838\n",
      "Epoch 160/200\n",
      "164/164 - 0s - loss: 0.2598 - accuracy: 0.8829\n",
      "Epoch 161/200\n",
      "164/164 - 0s - loss: 0.2580 - accuracy: 0.8858\n",
      "Epoch 162/200\n",
      "164/164 - 0s - loss: 0.2637 - accuracy: 0.8802\n",
      "Epoch 163/200\n",
      "164/164 - 0s - loss: 0.2597 - accuracy: 0.8808\n",
      "Epoch 164/200\n",
      "164/164 - 0s - loss: 0.2546 - accuracy: 0.8817\n",
      "Epoch 165/200\n",
      "164/164 - 0s - loss: 0.2561 - accuracy: 0.8831\n",
      "Epoch 166/200\n",
      "164/164 - 0s - loss: 0.2557 - accuracy: 0.8844\n",
      "Epoch 167/200\n",
      "164/164 - 0s - loss: 0.2613 - accuracy: 0.8825\n",
      "Epoch 168/200\n",
      "164/164 - 0s - loss: 0.2572 - accuracy: 0.8873\n",
      "Epoch 169/200\n",
      "164/164 - 0s - loss: 0.2628 - accuracy: 0.8837\n",
      "Epoch 170/200\n",
      "164/164 - 0s - loss: 0.2601 - accuracy: 0.8819\n",
      "Epoch 171/200\n",
      "164/164 - 0s - loss: 0.2527 - accuracy: 0.8869\n",
      "Epoch 172/200\n",
      "164/164 - 0s - loss: 0.2666 - accuracy: 0.8798\n",
      "Epoch 173/200\n",
      "164/164 - 0s - loss: 0.2571 - accuracy: 0.8816\n",
      "Epoch 174/200\n",
      "164/164 - 0s - loss: 0.2526 - accuracy: 0.8838\n",
      "Epoch 175/200\n",
      "164/164 - 0s - loss: 0.2517 - accuracy: 0.8865\n",
      "Epoch 176/200\n",
      "164/164 - 0s - loss: 0.2641 - accuracy: 0.8865\n",
      "Epoch 177/200\n",
      "164/164 - 0s - loss: 0.2550 - accuracy: 0.8846\n",
      "Epoch 178/200\n",
      "164/164 - 0s - loss: 0.2559 - accuracy: 0.8806\n",
      "Epoch 179/200\n",
      "164/164 - 0s - loss: 0.2519 - accuracy: 0.8844\n",
      "Epoch 180/200\n",
      "164/164 - 0s - loss: 0.2587 - accuracy: 0.8825\n",
      "Epoch 181/200\n",
      "164/164 - 0s - loss: 0.2589 - accuracy: 0.8837\n",
      "Epoch 182/200\n",
      "164/164 - 0s - loss: 0.2532 - accuracy: 0.8854\n",
      "Epoch 183/200\n",
      "164/164 - 0s - loss: 0.2575 - accuracy: 0.8842\n",
      "Epoch 184/200\n",
      "164/164 - 0s - loss: 0.2676 - accuracy: 0.8737\n",
      "Epoch 185/200\n",
      "164/164 - 0s - loss: 0.2628 - accuracy: 0.8800\n",
      "Epoch 186/200\n",
      "164/164 - 0s - loss: 0.2511 - accuracy: 0.8844\n",
      "Epoch 187/200\n",
      "164/164 - 0s - loss: 0.2581 - accuracy: 0.8810\n",
      "Epoch 188/200\n",
      "164/164 - 0s - loss: 0.2538 - accuracy: 0.8842\n",
      "Epoch 189/200\n",
      "164/164 - 0s - loss: 0.2547 - accuracy: 0.8867\n",
      "Epoch 190/200\n",
      "164/164 - 0s - loss: 0.2532 - accuracy: 0.8875\n",
      "Epoch 191/200\n",
      "164/164 - 0s - loss: 0.2528 - accuracy: 0.8846\n",
      "Epoch 192/200\n",
      "164/164 - 0s - loss: 0.2539 - accuracy: 0.8863\n",
      "Epoch 193/200\n",
      "164/164 - 0s - loss: 0.2500 - accuracy: 0.8882\n",
      "Epoch 194/200\n",
      "164/164 - 0s - loss: 0.2562 - accuracy: 0.8840\n",
      "Epoch 195/200\n",
      "164/164 - 0s - loss: 0.2514 - accuracy: 0.8840\n",
      "Epoch 196/200\n",
      "164/164 - 0s - loss: 0.2489 - accuracy: 0.8899\n",
      "Epoch 197/200\n",
      "164/164 - 0s - loss: 0.2517 - accuracy: 0.8877\n",
      "Epoch 198/200\n",
      "164/164 - 0s - loss: 0.2485 - accuracy: 0.8896\n",
      "Epoch 199/200\n",
      "164/164 - 0s - loss: 0.2517 - accuracy: 0.8892\n",
      "Epoch 200/200\n",
      "164/164 - 0s - loss: 0.2602 - accuracy: 0.8840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1930569b2e8>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the adjusted model\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_categorical,\n",
    "    epochs=200,\n",
    "    shuffle=True,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 - 0s - loss: 0.3204 - accuracy: 0.8976\n",
      "Normal Neural Network - Loss: 0.32036757469177246, Accuracy: 0.8975972533226013\n"
     ]
    }
   ],
   "source": [
    "# Qunatify the adjusted model\n",
    "model_loss, model_accuracy = model.evaluate(X_test_scaled, y_test_categorical, verbose=2)\n",
    "print(f\"Normal Neural Network - Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "     CONFIRMED       0.80      0.78      0.79       411\n",
      "FALSE POSITIVE       0.83      0.82      0.82       484\n",
      "     CANDIDATE       0.98      1.00      0.99       853\n",
      "\n",
      "      accuracy                           0.90      1748\n",
      "     macro avg       0.87      0.87      0.87      1748\n",
      "  weighted avg       0.90      0.90      0.90      1748\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate classification report\n",
    "predictions = np.argmax(model.predict(X_test_scaled), axis=-1)\n",
    "print(classification_report(encoded_y_test, predictions,\n",
    "                            target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 540x540 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a confusion matrix to visualise the performance of the adjusted model\n",
    "predictions = np.argmax(model.predict(X_test_scaled), axis=-1)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_true=encoded_y_test, y_pred=predictions)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.5, 7.5))\n",
    "\n",
    "ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n",
    "\n",
    "for i in range(conf_matrix.shape[0]):\n",
    "    for j in range(conf_matrix.shape[1]):\n",
    "        ax.text(x=j, y=i, s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n",
    "\n",
    "plt.title('Confusion Matrix', fontsize=18)\n",
    "plt.xlabel('Predictions', fontsize=18)\n",
    "ax.set_xticks([0,1,2])\n",
    "ax.set_xticklabels(target_names)\n",
    "plt.ylabel('Actuals', fontsize=18)\n",
    "ax.set_yticks([0,1,2])\n",
    "ax.set_yticklabels(target_names)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "filename = 'Model 2 - Deep_learning.h5'\n",
    "model.save(filename)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "dev"
  },
  "kernelspec": {
   "display_name": "Python [conda env:PythonData] *",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "nteract": {
   "version": "0.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
